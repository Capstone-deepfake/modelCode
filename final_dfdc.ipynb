{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6CYd3WUmtvz",
        "outputId": "55e77ceb-af1d-444d-af79-9087ee0a4427"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EFKkWyVemVl7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import concurrent.futures"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "input_file_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/DFDC/fake'\n",
        "output_file_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/DFDC/real'\n",
        "meta_data_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup'\n",
        "checkpoint_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/checkpoints'\n",
        "frames_required = 100\n",
        "valid_csv_path = f'{meta_data_path}/valid_videos.csv'\n",
        "# valid_csv_path = f'{meta_data_path}/valid_videos_old.csv'"
      ],
      "metadata": {
        "id": "VbCyL9WXmW1I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 기존 csv 파일 경로\n",
        "csv_path = f'{meta_data_path}/valid_videos_old.csv'\n",
        "\n",
        "# 새로운 기본 경로\n",
        "base_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/DFDC'\n",
        "\n",
        "# CSV 읽기 (헤더 있음)\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "def replace_path(old_path):\n",
        "    filename = os.path.basename(old_path)\n",
        "    if 'real' in old_path.lower():\n",
        "        return os.path.join(base_path, 'real', filename)\n",
        "    elif 'fake' in old_path.lower():\n",
        "        return os.path.join(base_path, 'fake', filename)\n",
        "    else:\n",
        "        # real/fake 단어가 없으면 원래 경로 유지하거나 처리\n",
        "        return old_path\n",
        "\n",
        "df['video_path'] = df['video_path'].apply(replace_path)\n",
        "\n",
        "df.to_csv(f'{meta_data_path}/valid_videos.csv', index=False)\n",
        "\n",
        "print(\"[INFO] 경로 변경 완료. valid_video_updated.csv 파일로 저장되었습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "WEId38SAqGJQ",
        "outputId": "4780a35c-9d38-4d44-c051-032109d72ce9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'meta_data_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-acd006292397>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 기존 csv 파일 경로\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{meta_data_path}/valid_videos_old.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 새로운 기본 경로\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'meta_data_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform\n",
        "im_size = 112\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Frame extractor\n",
        "def frame_extract(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    success, image = cap.read()\n",
        "    while success:\n",
        "        yield image\n",
        "        success, image = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "# Validate a single video\n",
        "def validate_and_count(video_path, transform, required_frames=20):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        cap.release()\n",
        "        if total_frames < frames_required:\n",
        "            return None\n",
        "\n",
        "        frames = []\n",
        "        for i, frame in enumerate(frame_extract(video_path)):\n",
        "            if frame is None: break\n",
        "            frames.append(transform(frame))\n",
        "            if len(frames) == required_frames:\n",
        "                break\n",
        "        if len(frames) < required_frames:\n",
        "            return None\n",
        "        return video_path, total_frames\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Step 1 & 2: Validate and filter videos (with CSV caching)\n",
        "video_files = glob.glob(f'{input_file_path}/*.mp4') + glob.glob(f'{output_file_path}/*.mp4')\n",
        "print(f\"[INFO] Total videos before validation: {len(video_files)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwWPLf1hmae1",
        "outputId": "6768815a-b90a-497d-9b28-578220e9fb2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Total videos before validation: 3085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(valid_csv_path):\n",
        "    print(\"[INFO] Loading validated video list from CSV...\")\n",
        "    valid_df = pd.read_csv(valid_csv_path)\n",
        "    valid_videos = valid_df['video_path'].tolist()\n",
        "    frame_counts = valid_df['frame_count'].tolist()\n",
        "else:\n",
        "    print(\"[INFO] Validating videos...\")\n",
        "    valid_videos = []\n",
        "    frame_counts = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        futures = [executor.submit(validate_and_count, v, transform) for v in video_files]\n",
        "        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Validating videos\"):\n",
        "            result = f.result()\n",
        "            if result:\n",
        "                valid_videos.append(result[0])\n",
        "                frame_counts.append(result[1])\n",
        "    valid_df = pd.DataFrame({\n",
        "        'video_path': valid_videos,\n",
        "        'frame_count': frame_counts\n",
        "    })\n",
        "    valid_df.to_csv(valid_csv_path, index=False)\n",
        "    print(f\"[INFO] Saved {len(valid_videos)} valid videos to valid_videos.csv\")\n",
        "\n",
        "print(f\"[INFO] Valid videos: {len(valid_videos)} | Avg frames: {np.mean(frame_counts):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBjmkEwkmgcO",
        "outputId": "d457c8b9-9963-4704-d70b-279d2400a357"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading validated video list from CSV...\n",
            "[INFO] Valid videos: 3064 | Avg frames: 147.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Dataset and Dataloader\n",
        "label_df = pd.read_csv(f'{meta_data_path}/metadata.csv', sep='\\t', names=[\"file\", \"label\"])\n",
        "# label_df = label_df.dropna(subset=[\"file\",\"label\"])\n",
        "\n",
        "# label_dict = {row[\"file\"]: 0 if str(row[\"label\"]).strip().lower() == \"fake\" else 1 for _, row in label_df.iterrows()}\n",
        "\n",
        "print(label_df.columns)\n",
        "# dropna와 dict 생성\n",
        "label_df = label_df.dropna(subset=[\"file\", \"label\"])\n",
        "label_df[\"file\"] = label_df[\"file\"].apply(lambda x: os.path.splitext(os.path.basename(str(x).strip()))[0])\n",
        "\n",
        "label_dict = {\n",
        "    row[\"file\"]: 0 if str(row[\"label\"]).strip().lower() == \"fake\" else 1\n",
        "    for _, row in label_df.iterrows()\n",
        "}\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, label_dict, sequence_length=10, transform=None):\n",
        "        self.video_paths = video_paths\n",
        "        self.label_dict = label_dict\n",
        "        self.sequence_length = sequence_length\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        frames = []\n",
        "\n",
        "        for frame in frame_extract(video_path):\n",
        "            if frame is None: continue\n",
        "            frames.append(self.transform(frame))\n",
        "            if len(frames) == self.sequence_length:\n",
        "                break\n",
        "        if len(frames) == 0:\n",
        "            raise RuntimeError(f\"No frames extracted from video: {video_path}\")\n",
        "\n",
        "        # frames = torch.stack(frames[:self.sequence_length])\n",
        "        # label = self.label_dict.get(os.path.basename(video_path), 0)\n",
        "\n",
        "        file_key = os.path.splitext(os.path.basename(video_path))[0]\n",
        "        label = self.label_dict.get(file_key, 0)  # fallback은 0\n",
        "\n",
        "        return frames, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88p31sm4miyT",
        "outputId": "860e6c1d-55f6-4030-eb56-38c4be7a1356"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['file', 'label'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, label_dict, sequence_length=10, transform=None):\n",
        "        self.video_paths = video_paths\n",
        "        self.label_dict = label_dict\n",
        "        self.sequence_length = sequence_length\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        frames = []\n",
        "\n",
        "        for frame in frame_extract(video_path):\n",
        "            if frame is None: continue\n",
        "            frames.append(self.transform(frame))\n",
        "            if len(frames) == self.sequence_length:\n",
        "                break\n",
        "        if len(frames) == 0:\n",
        "            raise RuntimeError(f\"No frames extracted from video: {video_path}\")\n",
        "\n",
        "        # Uncomment this line to stack the list of frame tensors into a single tensor\n",
        "        frames = torch.stack(frames[:self.sequence_length])\n",
        "        # label = self.label_dict.get(os.path.basename(video_path), 0)\n",
        "\n",
        "        file_key = os.path.splitext(os.path.basename(video_path))[0]\n",
        "        label = self.label_dict.get(file_key, 0)  # fallback은 0\n",
        "\n",
        "        return frames, label"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JKOf86veOYqK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "label_counts = Counter(label_dict.values())\n",
        "print(f\"Updated label distribution: {label_counts}\")\n",
        "# 기대 결과: Counter({0: xxx, 1: yyy})  // fake와 real 적절히 섞여 있어야 함\n",
        "\n",
        "print(f\"Train size: {len(train_videos)}\")\n",
        "print(f\"Val size: {len(val_videos)}\")\n",
        "print(f\"Intersection: {len(set(train_videos) & set(val_videos))}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCT9LmqkNSg6",
        "outputId": "d5e1327d-00b9-42cf-dfd6-916ab4de4a6b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated label distribution: Counter({1: 2513, 0: 2362})\n",
            "Train size: 2144\n",
            "Val size: 920\n",
            "Intersection: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV 파일을 제대로 읽고 있는지 확인\n",
        "# label_df = pd.read_csv(f'{meta_data_path}/metadata.csv')\n",
        "# print(label_df.head())\n",
        "# print(label_df.columns)"
      ],
      "metadata": {
        "id": "u5m27yAhNakv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Val Split\n",
        "random.shuffle(valid_videos)\n",
        "train_split = int(0.7 * len(valid_videos))\n",
        "train_videos = valid_videos[:train_split]\n",
        "val_videos = valid_videos[train_split:]\n",
        "\n",
        "train_data = VideoDataset(train_videos, label_dict, transform=transform)\n",
        "val_data = VideoDataset(val_videos, label_dict, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=6)\n",
        "val_loader = DataLoader(val_data, batch_size=4, shuffle=False, num_workers=6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AZv7ftJmjj3",
        "outputId": "dcdead4b-202b-494e-e225-3c6bb6a50131"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes=2, latent_dim=1280, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
        "        super(Model, self).__init__()\n",
        "        base = models.efficientnet_b0(pretrained=True)\n",
        "        self.features = base.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x).view(B, T, -1)\n",
        "        x, _ = self.lstm(x)\n",
        "        out = self.fc(self.dropout(x.mean(dim=1)))\n",
        "        return out\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
      ],
      "metadata": {
        "id": "TzuJGc92mliG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train & Evaluate\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    _, preds = outputs.max(1)\n",
        "    return (preds == targets).float().mean().item() * 100\n",
        "\n",
        "def train_epoch(epoch, model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        acc = calculate_accuracy(outputs, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += acc\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            eta = elapsed / (i + 1) * (len(loader) - i - 1)\n",
        "            print(f\"\\r[Train] Epoch {epoch} | Batch {i+1}/{len(loader)} | Loss: {loss.item():.4f} | Acc: {acc:.2f}% | ETA: {eta:.1f}s\", end=\"\")\n",
        "\n",
        "    print()\n",
        "    return total_loss / len(loader), total_acc / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "            acc = calculate_accuracy(outputs, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc\n",
        "\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            y_pred.extend(preds)\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "\n",
        "    return total_loss / len(loader), total_acc / len(loader), y_true, y_pred\n"
      ],
      "metadata": {
        "id": "c0Qmx3MWmnM2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Training Loop\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f\"\\n[INFO] Starting Epoch {epoch}/{epochs}\")\n",
        "    train_loss, train_acc = train_epoch(epoch, model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_acc, y_true, y_pred = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"[INFO] Epoch {epoch} Completed | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "_chFHIcTmpfb",
        "outputId": "b23574bb-4dae-4afb-bd04-e793277afba4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INFO] Starting Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-b42dcfb64213>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[INFO] Starting Epoch {epoch}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-fc1d4040d7d9>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, model, loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(path, model, optimizer):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        best_val_acc = checkpoint['val_acc']\n",
        "        print(f\"[INFO] Loaded checkpoint from epoch {start_epoch} with best val acc: {best_val_acc:.2f}%\")\n",
        "        return start_epoch, best_val_acc\n",
        "    else:\n",
        "        print(\"[INFO] No checkpoint found. Starting from scratch.\")\n",
        "        return 0, 0.0\n"
      ],
      "metadata": {
        "id": "H-6sVBNoYui1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 설정\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "epochs = 7\n",
        "checkpoint_path = 'best_checkpoint.pth'\n",
        "\n",
        "# 체크포인트 로딩\n",
        "start_epoch, best_val_acc = load_checkpoint(checkpoint_path, model, optimizer)\n",
        "\n",
        "# 이어서 학습\n",
        "for epoch in range(start_epoch + 1, epochs + 1):\n",
        "    print(f\"\\n[INFO] Starting Epoch {epoch}/{epochs}\")\n",
        "    train_loss, train_acc = train_epoch(epoch, model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_acc, y_true, y_pred = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"[INFO] Epoch {epoch} Completed | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # 최고 성능 모델 저장\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': best_val_acc\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"[INFO] Saved checkpoint at Epoch {epoch} with Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(\"[INFO] Training Complete\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0AQ5hXyYxQV",
        "outputId": "b46816e7-8237-49f8-b3e2-e38646bc86e8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] No checkpoint found. Starting from scratch.\n",
            "\n",
            "[INFO] Starting Epoch 1/7\n",
            "[Train] Epoch 1 | Batch 530/536 | Loss: 0.5608 | Acc: 50.00% | ETA: 23.5s\n",
            "[INFO] Epoch 1 Completed | Train Acc: 67.02% | Val Acc: 72.83%\n",
            "[INFO] Saved checkpoint at Epoch 1 with Val Acc: 72.83%\n",
            "\n",
            "[INFO] Starting Epoch 2/7\n",
            "[Train] Epoch 2 | Batch 530/536 | Loss: 0.7446 | Acc: 50.00% | ETA: 23.6s\n",
            "[INFO] Epoch 2 Completed | Train Acc: 76.17% | Val Acc: 78.70%\n",
            "[INFO] Saved checkpoint at Epoch 2 with Val Acc: 78.70%\n",
            "\n",
            "[INFO] Starting Epoch 3/7\n",
            "[Train] Epoch 3 | Batch 530/536 | Loss: 0.1196 | Acc: 100.00% | ETA: 23.3s\n",
            "[INFO] Epoch 3 Completed | Train Acc: 81.53% | Val Acc: 83.26%\n",
            "[INFO] Saved checkpoint at Epoch 3 with Val Acc: 83.26%\n",
            "\n",
            "[INFO] Starting Epoch 4/7\n",
            "[Train] Epoch 4 | Batch 530/536 | Loss: 0.5375 | Acc: 50.00% | ETA: 23.6s\n",
            "[INFO] Epoch 4 Completed | Train Acc: 85.21% | Val Acc: 79.46%\n",
            "\n",
            "[INFO] Starting Epoch 5/7\n",
            "[Train] Epoch 5 | Batch 530/536 | Loss: 0.0261 | Acc: 100.00% | ETA: 23.5s\n",
            "[INFO] Epoch 5 Completed | Train Acc: 86.57% | Val Acc: 82.50%\n",
            "\n",
            "[INFO] Starting Epoch 6/7\n",
            "[Train] Epoch 6 | Batch 530/536 | Loss: 0.0795 | Acc: 100.00% | ETA: 23.5s\n",
            "[INFO] Epoch 6 Completed | Train Acc: 88.71% | Val Acc: 82.17%\n",
            "\n",
            "[INFO] Starting Epoch 7/7\n",
            "[Train] Epoch 7 | Batch 530/536 | Loss: 0.0223 | Acc: 100.00% | ETA: 23.0s\n",
            "[INFO] Epoch 7 Completed | Train Acc: 92.77% | Val Acc: 84.13%\n",
            "[INFO] Saved checkpoint at Epoch 7 with Val Acc: 84.13%\n",
            "[INFO] Training Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 모델 저장\n",
        "torch.save(model, 'final_model.pth')"
      ],
      "metadata": {
        "id": "3NoCRAA3Y3Ug"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Confusion Matrix\n",
        "def print_confusion(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"\\n[CONFUSION MATRIX]:\\n\", cm)\n",
        "    print(\"\\n[CLASSIFICATION REPORT]:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=[\"Fake\", \"Real\"]))\n",
        "    sn.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "print_confusion(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "WnrTzMDWoIKf",
        "outputId": "95eb8624-f39d-4e01-9410-10026076584a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[CONFUSION MATRIX]:\n",
            " [[350  89]\n",
            " [ 57 424]]\n",
            "\n",
            "[CLASSIFICATION REPORT]:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.86      0.80      0.83       439\n",
            "        Real       0.83      0.88      0.85       481\n",
            "\n",
            "    accuracy                           0.84       920\n",
            "   macro avg       0.84      0.84      0.84       920\n",
            "weighted avg       0.84      0.84      0.84       920\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}