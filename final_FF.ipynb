{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNQEQewTlkUNhZK7kIFdVIL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6CYd3WUmtvz","executionInfo":{"status":"ok","timestamp":1747832959425,"user_tz":-540,"elapsed":21065,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"7e254696-551d-4cbe-e429-ae5a66d5779f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFKkWyVemVl7"},"outputs":[],"source":["import os\n","import glob\n","import cv2\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from tqdm import tqdm\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","import time\n","import concurrent.futures"]},{"cell_type":"code","source":["# Paths\n","input_file_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/FF/fake'\n","output_file_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/FF/real'\n","meta_data_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup'\n","checkpoint_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/checkpoints/best_checkpoint_FF.pth'\n","frames_required = 100\n","# valid_csv_path = f'{meta_data_path}/valid_videos.csv'\n","# valid_csv_path = f'{meta_data_path}/valid_videos_old.csv'"],"metadata":{"id":"VbCyL9WXmW1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# 기존 csv 파일 경로\n","csv_path = f'{meta_data_path}/valid_videos.csv'\n","\n","# 새로운 기본 경로\n","base_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/FF'\n","\n","# CSV 읽기 (헤더 있음)\n","df = pd.read_csv(csv_path)\n","\n","def replace_path(old_path):\n","    filename = os.path.basename(old_path)\n","    if 'real' in old_path.lower():\n","        return os.path.join(base_path, 'real', filename)\n","    elif 'fake' in old_path.lower():\n","        return os.path.join(base_path, 'fake', filename)\n","    else:\n","        # real/fake 단어가 없으면 원래 경로 유지하거나 처리\n","        return old_path\n","\n","df['video_path'] = df['video_path'].apply(replace_path)\n","\n","df.to_csv(f'{meta_data_path}/valid_videos_FF.csv', index=False)\n","\n","print(\"[INFO] 경로 변경 완료. valid_video_updated.csv 파일로 저장되었습니다.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WEId38SAqGJQ","executionInfo":{"status":"ok","timestamp":1747831274780,"user_tz":-540,"elapsed":313,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"408ebf29-8e28-4bad-b62b-b164cc1e2863"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] 경로 변경 완료. valid_video_updated.csv 파일로 저장되었습니다.\n"]}]},{"cell_type":"code","source":["valid_csv_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/filtered.csv'"],"metadata":{"id":"8Cr7ErrjY2Ml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# CSV 파일 경로 (수정 필요)\n","csv_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/valid_videos_FF.csv'\n","df = pd.read_csv(csv_path)\n","\n","# validation 폴더 경로 (수정 필요)\n","validation_dir = '/content/drive/MyDrive/class/capstone_com/datasetbackup/FF/validation'\n","\n","# validation 폴더 내 파일/폴더 이름들 (확장자 포함 여부에 따라 조정)\n","validation_files = set(os.listdir(validation_dir))\n","\n","# video_path에서 마지막 경로만 추출 (폴더명 or 파일명)\n","df['video_name'] = df['video_path'].apply(lambda x: os.path.basename(x))\n","\n","# validation에 이미 있는 항목은 제외\n","filtered_df = df[~df['video_name'].isin(validation_files)]\n","\n","# 필요시 결과 저장\n","filtered_df.to_csv('/content/drive/MyDrive/class/capstone_com/datasetbackup/filtered.csv', index=False)\n"],"metadata":{"id":"Bhm5c-_95fXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform\n","im_size = 112\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((im_size, im_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225])\n","])\n","\n","# Frame extractor\n","def frame_extract(path):\n","    cap = cv2.VideoCapture(path)\n","    success, image = cap.read()\n","    while success:\n","        yield image\n","        success, image = cap.read()\n","    cap.release()\n","\n","# Validate a single video\n","def validate_and_count(video_path, transform, required_frames=20):\n","    try:\n","        cap = cv2.VideoCapture(video_path)\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        cap.release()\n","        if total_frames < frames_required:\n","            return None\n","\n","        frames = []\n","        for i, frame in enumerate(frame_extract(video_path)):\n","            if frame is None: break\n","            frames.append(transform(frame))\n","            if len(frames) == required_frames:\n","                break\n","        if len(frames) < required_frames:\n","            return None\n","        return video_path, total_frames\n","    except:\n","        return None\n","\n","# Step 1 & 2: Validate and filter videos (with CSV caching)\n","video_files = glob.glob(f'{input_file_path}/*.mp4') + glob.glob(f'{output_file_path}/*.mp4')\n","print(f\"[INFO] Total videos before validation: {len(video_files)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwWPLf1hmae1","executionInfo":{"status":"ok","timestamp":1747832979440,"user_tz":-540,"elapsed":5945,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"8952f5b7-80fd-4f04-a04e-3d5696d7dca3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Total videos before validation: 1929\n"]}]},{"cell_type":"code","source":["if os.path.exists(valid_csv_path):\n","    print(\"[INFO] Loading validated video list from CSV...\")\n","    valid_df = pd.read_csv(valid_csv_path)\n","    valid_videos = valid_df['video_path'].tolist()\n","    frame_counts = valid_df['frame_count'].tolist()\n","else:\n","    print(\"[INFO] Validating videos...\")\n","    valid_videos = []\n","    frame_counts = []\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n","        futures = [executor.submit(validate_and_count, v, transform) for v in video_files]\n","        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Validating videos\"):\n","            result = f.result()\n","            if result:\n","                valid_videos.append(result[0])\n","                frame_counts.append(result[1])\n","    valid_df = pd.DataFrame({\n","        'video_path': valid_videos,\n","        'frame_count': frame_counts\n","    })\n","    valid_df.to_csv(valid_csv_path, index=False)\n","    print(f\"[INFO] Saved {len(valid_videos)} valid videos to valid_videos.csv\")\n","\n","print(f\"[INFO] Valid videos: {len(valid_videos)} | Avg frames: {np.mean(frame_counts):.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XBjmkEwkmgcO","outputId":"1aba43a8-7f84-4f8b-e826-ade4d0890e9b","executionInfo":{"status":"ok","timestamp":1747832979926,"user_tz":-540,"elapsed":484,"user":{"displayName":"이은서","userId":"09202701988006085748"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Loading validated video list from CSV...\n","[INFO] Valid videos: 1727 | Avg frames: 148.00\n"]}]},{"cell_type":"code","source":["# Step 3: Dataset and Dataloader\n","label_df = pd.read_csv(f'{meta_data_path}/metadata.csv', sep='\\t', names=[\"file\", \"label\"])\n","# label_df = label_df.dropna(subset=[\"file\",\"label\"])\n","\n","# label_dict = {row[\"file\"]: 0 if str(row[\"label\"]).strip().lower() == \"fake\" else 1 for _, row in label_df.iterrows()}\n","\n","print(label_df.columns)\n","# dropna와 dict 생성\n","label_df = label_df.dropna(subset=[\"file\", \"label\"])\n","label_df[\"file\"] = label_df[\"file\"].apply(lambda x: os.path.splitext(os.path.basename(str(x).strip()))[0])\n","\n","label_dict = {\n","    row[\"file\"]: 0 if str(row[\"label\"]).strip().lower() == \"fake\" else 1\n","    for _, row in label_df.iterrows()\n","}\n","\n","class VideoDataset(Dataset):\n","    def __init__(self, video_paths, label_dict, sequence_length=10, transform=None):\n","        self.video_paths = video_paths\n","        self.label_dict = label_dict\n","        self.sequence_length = sequence_length\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.video_paths[idx]\n","        frames = []\n","\n","        for frame in frame_extract(video_path):\n","            if frame is None: continue\n","            frames.append(self.transform(frame))\n","            if len(frames) == self.sequence_length:\n","                break\n","        if len(frames) == 0:\n","            raise RuntimeError(f\"No frames extracted from video: {video_path}\")\n","\n","        # frames = torch.stack(frames[:self.sequence_length])\n","        # label = self.label_dict.get(os.path.basename(video_path), 0)\n","\n","        file_key = os.path.splitext(os.path.basename(video_path))[0]\n","        label = self.label_dict.get(file_key, 0)  # fallback은 0\n","\n","        return frames, label"],"metadata":{"id":"88p31sm4miyT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747832980474,"user_tz":-540,"elapsed":545,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"e12e987c-817d-42a2-ff55-18f45da53c39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['file', 'label'], dtype='object')\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from torch.utils.data import Dataset\n","import cv2\n","\n","class VideoDataset(Dataset):\n","    def __init__(self, video_paths, label_dict, sequence_length=10, transform=None):\n","        self.sequence_length = sequence_length\n","        self.transform = transform\n","        self.label_dict = label_dict\n","\n","        # ✅ 유효한 영상만 필터링\n","        self.video_paths = []\n","        for path in video_paths:\n","            if self.is_valid_video(path):\n","                self.video_paths.append(path)\n","            else:\n","                print(f\"[Warning] Invalid video skipped: {path}\")\n","\n","    def is_valid_video(self, video_path):\n","        cap = cv2.VideoCapture(video_path)\n","        ret, _ = cap.read()\n","        cap.release()\n","        return ret  # 첫 프레임을 읽을 수 있으면 True\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.video_paths[idx]\n","        frames = []\n","\n","        for frame in frame_extract(video_path):\n","            if frame is None:\n","                continue\n","            frames.append(self.transform(frame))\n","            if len(frames) == self.sequence_length:\n","                break\n","\n","        if len(frames) == 0:\n","            raise RuntimeError(f\"No frames extracted from video: {video_path}\")\n","\n","        frames = torch.stack(frames[:self.sequence_length])\n","\n","        file_key = os.path.splitext(os.path.basename(video_path))[0]\n","        label = self.label_dict.get(file_key, 0)\n","\n","        return frames, label\n"],"metadata":{"id":"7SNkDS92ykLi"},"execution_count":null,"outputs":[]},{"source":["class VideoDataset(Dataset):\n","    def __init__(self, video_paths, label_dict, sequence_length=10, transform=None):\n","        self.video_paths = video_paths\n","        self.label_dict = label_dict\n","        self.sequence_length = sequence_length\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.video_paths[idx]\n","        frames = []\n","\n","        for frame in frame_extract(video_path):\n","            if frame is None: continue\n","            frames.append(self.transform(frame))\n","            if len(frames) == self.sequence_length:\n","                break\n","        if len(frames) == 0:\n","            raise RuntimeError(f\"No frames extracted from video: {video_path}\")\n","\n","        # Uncomment this line to stack the list of frame tensors into a single tensor\n","        frames = torch.stack(frames[:self.sequence_length])\n","        # label = self.label_dict.get(os.path.basename(video_path), 0)\n","\n","        file_key = os.path.splitext(os.path.basename(video_path))[0]\n","        label = self.label_dict.get(file_key, 0)  # fallback은 0\n","\n","        return frames, label"],"cell_type":"code","metadata":{"id":"JKOf86veOYqK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","label_counts = Counter(label_dict.values())\n","print(f\"Updated label distribution: {label_counts}\")\n","# 기대 결과: Counter({0: xxx, 1: yyy})  // fake와 real 적절히 섞여 있어야 함\n","\n","print(f\"Train size: {len(train_videos)}\")\n","print(f\"Val size: {len(val_videos)}\")\n","print(f\"Intersection: {len(set(train_videos) & set(val_videos))}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iCT9LmqkNSg6","executionInfo":{"status":"ok","timestamp":1747831509128,"user_tz":-540,"elapsed":14,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"6399557d-b39f-420d-f74f-911833b0374d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated label distribution: Counter({1: 2513, 0: 2362})\n","Train size: 1250\n","Val size: 537\n","Intersection: 0\n"]}]},{"cell_type":"code","source":["# CSV 파일을 제대로 읽고 있는지 확인\n","# label_df = pd.read_csv(f'{meta_data_path}/metadata.csv')\n","# print(label_df.head())\n","# print(label_df.columns)"],"metadata":{"id":"u5m27yAhNakv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train/Val Split\n","random.shuffle(valid_videos)\n","train_split = int(0.7 * len(valid_videos))\n","train_videos = valid_videos[:train_split]\n","val_videos = valid_videos[train_split:]\n","\n","train_data = VideoDataset(train_videos, label_dict, transform=transform)\n","val_data = VideoDataset(val_videos, label_dict, transform=transform)\n","\n","train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=6)\n","val_loader = DataLoader(val_data, batch_size=4, shuffle=False, num_workers=6)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_AZv7ftJmjj3","executionInfo":{"status":"ok","timestamp":1747832980607,"user_tz":-540,"elapsed":34,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"c2ad4012-e444-4868-bea3-e8becff67272"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Step 4: Model\n","class Model(nn.Module):\n","    def __init__(self, num_classes=2, latent_dim=1280, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n","        super(Model, self).__init__()\n","        base = models.efficientnet_b0(pretrained=True)\n","        self.features = base.features\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n","        self.fc = nn.Linear(hidden_dim, num_classes)\n","        self.dropout = nn.Dropout(0.4)\n","\n","    def forward(self, x):\n","        B, T, C, H, W = x.shape\n","        x = x.view(B * T, C, H, W)\n","        x = self.features(x)\n","        x = self.avgpool(x).view(B, T, -1)\n","        x, _ = self.lstm(x)\n","        out = self.fc(self.dropout(x.mean(dim=1)))\n","        return out\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"],"metadata":{"id":"TzuJGc92mliG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","\n","# ✅ Focal Loss 정의\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=1, gamma=2):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.ce = nn.CrossEntropyLoss(reduction='none')\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = self.ce(inputs, targets)\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n","        return focal_loss.mean()\n","\n","# ✅ 모델 정의: EfficientNet + Bi-LSTM\n","class Model(nn.Module):\n","    def __init__(self, num_classes=2, latent_dim=1280, lstm_layers=1, hidden_dim=2048, bidirectional=True):\n","        super(Model, self).__init__()\n","        base = models.efficientnet_b0(pretrained=True)\n","        self.features = base.features  # EfficientNetB0 feature extractor\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","\n","        self.bidirectional = bidirectional\n","        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional=bidirectional)\n","        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n","\n","        self.dropout = nn.Dropout(0.4)\n","        self.fc = nn.Linear(lstm_output_dim, num_classes)\n","\n","    def forward(self, x):\n","        # x shape: (B, T, C, H, W)\n","        B, T, C, H, W = x.shape\n","        x = x.view(B * T, C, H, W)\n","\n","        # CNN feature extraction\n","        x = self.features(x)\n","        x = self.avgpool(x)  # shape: (B*T, latent_dim, 1, 1)\n","        x = x.view(B, T, -1)  # shape: (B, T, latent_dim)\n","\n","        # LSTM\n","        x, _ = self.lstm(x)  # shape: (B, T, hidden_dim * 2)\n","\n","        # Temporal average pooling\n","        x = x.mean(dim=1)  # shape: (B, hidden_dim * 2)\n","\n","        # Fully connected\n","        x = self.dropout(x)\n","        out = self.fc(x)\n","        return out\n","\n","# ✅ 디바이스 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# ✅ 모델 초기화\n","model = Model().to(device)\n","\n","# ✅ 손실 함수: Focal Loss\n","criterion = FocalLoss(alpha=1, gamma=2)\n","\n","# ✅ 옵티마이저\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IWYmjFNvGBX_","executionInfo":{"status":"ok","timestamp":1747835169337,"user_tz":-540,"elapsed":1441,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"e048b784-f4ea-4d94-8c35-22e54c8c2b6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["# Step 5: Train & Evaluate\n","def calculate_accuracy(outputs, targets):\n","    _, preds = outputs.max(1)\n","    return (preds == targets).float().mean().item() * 100\n","\n","def train_epoch(epoch, model, loader, criterion, optimizer):\n","    model.train()\n","    total_loss, total_acc = 0, 0\n","    start_time = time.time()\n","\n","    for i, (x, y) in enumerate(loader):\n","        x, y = x.to(device), y.to(device)\n","        outputs = model(x)\n","        loss = criterion(outputs, y)\n","        acc = calculate_accuracy(outputs, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        total_acc += acc\n","\n","        if (i + 1) % 10 == 0:\n","            elapsed = time.time() - start_time\n","            eta = elapsed / (i + 1) * (len(loader) - i - 1)\n","            print(f\"\\r[Train] Epoch {epoch} | Batch {i+1}/{len(loader)} | Loss: {loss.item():.4f} | Acc: {acc:.2f}% | ETA: {eta:.1f}s\", end=\"\")\n","\n","    print()\n","    return total_loss / len(loader), total_acc / len(loader)\n","\n","def evaluate(model, loader, criterion):\n","    model.eval()\n","    total_loss, total_acc = 0, 0\n","    y_true, y_pred = [], []\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(device), y.to(device)\n","            outputs = model(x)\n","            loss = criterion(outputs, y)\n","            acc = calculate_accuracy(outputs, y)\n","\n","            total_loss += loss.item()\n","            total_acc += acc\n","\n","            preds = outputs.argmax(dim=1).cpu().numpy()\n","            y_pred.extend(preds)\n","            y_true.extend(y.cpu().numpy())\n","\n","    return total_loss / len(loader), total_acc / len(loader), y_true, y_pred\n"],"metadata":{"id":"c0Qmx3MWmnM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''# Step 6: Training Loop\n","train_losses, val_losses = [], []\n","train_accs, val_accs = [], []\n","\n","epochs = 10\n","for epoch in range(1, epochs + 1):\n","    print(f\"\\n[INFO] Starting Epoch {epoch}/{epochs}\")\n","    train_loss, train_acc = train_epoch(epoch, model, train_loader, criterion, optimizer)\n","    val_loss, val_acc, y_true, y_pred = evaluate(model, val_loader, criterion)\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    train_accs.append(train_acc)\n","    val_accs.append(val_acc)\n","\n","    print(f\"[INFO] Epoch {epoch} Completed | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n","\n","'''\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":633},"id":"_chFHIcTmpfb","executionInfo":{"status":"error","timestamp":1747825931727,"user_tz":-540,"elapsed":989,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"e546bcd2-f15d-4a32-cbcc-52215e7693f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[INFO] Starting Epoch 1/10\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-22-fcf752e78103>\", line 21, in __getitem__\n    raise RuntimeError(f\"No frames extracted from video: {video_path}\")\nRuntimeError: No frames extracted from video: /content/drive/MyDrive/class/capstone_com/datasetbackup/fake/019_018.mp4\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-b42dcfb64213>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[INFO] Starting Epoch {epoch}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-fc1d4040d7d9>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, model, loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-22-fcf752e78103>\", line 21, in __getitem__\n    raise RuntimeError(f\"No frames extracted from video: {video_path}\")\nRuntimeError: No frames extracted from video: /content/drive/MyDrive/class/capstone_com/datasetbackup/fake/019_018.mp4\n"]}]},{"cell_type":"code","source":["def load_checkpoint(path, model, optimizer):\n","    if os.path.exists(path):\n","        checkpoint = torch.load(path, map_location=device)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch']\n","        best_val_acc = checkpoint['val_acc']\n","        print(f\"[INFO] Loaded checkpoint from epoch {start_epoch} with best val acc: {best_val_acc:.2f}%\")\n","        return start_epoch, best_val_acc\n","    else:\n","        print(\"[INFO] No checkpoint found. Starting from scratch.\")\n","        return 0, 0.0\n"],"metadata":{"id":"H-6sVBNoYui1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 설정\n","train_losses, val_losses = [], []\n","train_accs, val_accs = [], []\n","epochs = 10\n","# checkpoint_path = 'best_checkpoint.pth'\n","\n","# 체크포인트 로딩\n","start_epoch, best_val_acc = load_checkpoint(checkpoint_path, model, optimizer)\n","\n","# 이어서 학습\n","for epoch in range(start_epoch + 1, epochs + 1):\n","    print(f\"\\n[INFO] Starting Epoch {epoch}/{epochs}\")\n","    train_loss, train_acc = train_epoch(epoch, model, train_loader, criterion, optimizer)\n","    val_loss, val_acc, y_true, y_pred = evaluate(model, val_loader, criterion)\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    train_accs.append(train_acc)\n","    val_accs.append(val_acc)\n","\n","    print(train_losses)\n","    print(f\"[INFO] Epoch {epoch} Completed | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n","\n","    # 최고 성능 모델 저장\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'val_acc': best_val_acc\n","        }\n","        torch.save(checkpoint, checkpoint_path)\n","        print(f\"[INFO] Saved checkpoint at Epoch {epoch} with Val Acc: {val_acc:.2f}%\")\n","\n","print(\"[INFO] Training Complete\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0AQ5hXyYxQV","outputId":"97230b35-8e63-4818-98b2-42a24178f3d3","executionInfo":{"status":"ok","timestamp":1747835336185,"user_tz":-540,"elapsed":1423,"user":{"displayName":"이은서","userId":"09202701988006085748"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Loaded checkpoint from epoch 10 with best val acc: 91.41%\n","[INFO] Training Complete\n"]}]},{"cell_type":"code","source":["# 전체 모델 저장\n","torch.save(model, '/content/drive/MyDrive/class/capstone_com/datasetbackup/final_model_FF.pth')"],"metadata":{"id":"3NoCRAA3Y3Ug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_device():\n","    if torch.backends.mps.is_available():\n","        print(\"MPS is available. Using MPS.\")\n","        device = torch.device(\"mps\")\n","    elif torch.cuda.is_available():\n","        print(\"CUDA is available. Using CUDA.\")\n","        device = torch.device(\"cuda\")\n","    else:\n","        print(\"CUDA and MPS not available. Using CPU.\")\n","        device = torch.device(\"cpu\")\n","    return device\n","\n","# 디바이스 설정\n","device = get_device()\n","print(f\"✅ Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRCVlmEHhFW1","executionInfo":{"status":"ok","timestamp":1747833594713,"user_tz":-540,"elapsed":63,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"6f6b4cf2-c679-42ca-fef4-b25f30019ebd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA is available. Using CUDA.\n","✅ Using device: cuda\n"]}]},{"cell_type":"code","source":["# Step 7: Confusion Matrix\n","def print_confusion(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    print(\"\\n[CONFUSION MATRIX]:\\n\", cm)\n","    print(\"\\n[CLASSIFICATION REPORT]:\")\n","    print(classification_report(y_true, y_pred, target_names=[\"Fake\", \"Real\"]))\n","    sn.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"Actual\")\n","    plt.title(\"Confusion Matrix\")\n","    plt.show()\n","\n","print_confusion(y_true, y_pred)"],"metadata":{"id":"WnrTzMDWoIKf","colab":{"base_uri":"https://localhost:8080/","height":740},"executionInfo":{"status":"ok","timestamp":1747835342535,"user_tz":-540,"elapsed":207,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"5e316432-21b2-4fb9-a045-6fa681a361ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[CONFUSION MATRIX]:\n"," [[214  19]\n"," [ 25 261]]\n","\n","[CLASSIFICATION REPORT]:\n","              precision    recall  f1-score   support\n","\n","        Fake       0.90      0.92      0.91       233\n","        Real       0.93      0.91      0.92       286\n","\n","    accuracy                           0.92       519\n","   macro avg       0.91      0.92      0.91       519\n","weighted avg       0.92      0.92      0.92       519\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","\n","# Import the Model class definition from earlier in your notebook\n","# Make sure the Model class is defined before this cell runs\n","# from your_model_file import Model # If your model is in a separate file\n","\n","# === 1. 설정 ===\n","validation_root = '/content/drive/MyDrive/class/capstone_com/datasetbackup/FF/validation'  # 여기를 실제 경로로 수정\n","frames_required = 100\n","label_map = {'real': 0, 'fake': 1}\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# === 2. 영상에서 일정 수의 프레임 추출 함수 ===\n","def extract_frames_from_video(video_path, num_frames=100):\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    frame_idxs = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n","\n","    frames = []\n","    for idx in range(total_frames):\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        if idx in frame_idxs:\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frames.append(frame)\n","    cap.release()\n","    return frames\n","\n","# === 3. Transform 정의 ===\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","# === 4. Dataset 정의 ===\n","class ValidationVideoDataset(Dataset):\n","    def __init__(self, root_dir, transform=None, frames_required=100):\n","        self.samples = []\n","        self.transform = transform\n","        self.frames_required = frames_required\n","\n","        for label_name in ['real', 'fake']:\n","            class_dir = os.path.join(root_dir, label_name)\n","            if not os.path.isdir(class_dir):\n","                print(f\"[Warning] Directory not found: {class_dir}\")\n","                continue\n","            for filename in os.listdir(class_dir):\n","                if filename.endswith('.mp4'):\n","                    self.samples.append((os.path.join(class_dir, filename), label_map[label_name]))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        video_path, label = self.samples[idx]\n","        frames = extract_frames_from_video(video_path, self.frames_required)\n","\n","        if not frames:\n","            # Handle case where no frames are extracted\n","            print(f\"[Warning] No frames extracted from {video_path}. Skipping.\")\n","            # You might want to return None and handle it in the DataLoader loop,\n","            # or return a dummy tensor/skip the sample during dataset creation.\n","            # For simplicity here, we raise an error, but a more robust approach\n","            # would be to filter out invalid videos during dataset initialization.\n","            raise RuntimeError(f\"No frames extracted from video: {video_path}\")\n","\n","\n","        # transform each frame\n","        transformed_frames = [self.transform(frame) for frame in frames]\n","        frames_tensor = torch.stack(transformed_frames)  # Shape: [T, C, H, W]\n","        return frames_tensor, label\n","\n","# === 5. 모델 불러오기 ===\n","# Instantiate the model with the same architecture as the saved checkpoint\n","# Ensure the Model class is defined in your notebook or imported\n","model = Model().to(device) # Create an instance of your Model class\n","\n","# Load the checkpoint dictionary\n","checkpoint = torch.load('/content/drive/MyDrive/class/capstone_com/datasetbackup/checkpoints/best_checkpoint_FF.pth', map_location=device)\n","\n","# Load the state_dict from the checkpoint into the model instance\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# === 6. DataLoader 만들기 ===\n","val_dataset = ValidationVideoDataset(validation_root, transform=transform, frames_required=frames_required)\n","val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n","\n","# === 7. 평가 ===\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for inputs, labels in tqdm(val_loader):\n","        # Ensure inputs have the correct shape for the model (B, T, C, H, W)\n","        # DataLoader with batch_size=1 will have shape [1, T, C, H, W] which is correct\n","        inputs = inputs.to(device)\n","\n","        # Labels from Dataset might be a list/int, ensure it's a tensor\n","        labels = torch.tensor([labels]).to(device) # Wrap labels in a list if batch_size=1\n","\n","        outputs = model(inputs)  # outputs shape: [1, 2]\n","        preds = torch.argmax(outputs, dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy()) # Extract scalar if batch_size=1\n","\n","# === 8. 결과 출력 ===\n","print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n","print(\"Classification Report:\")\n","print(classification_report(all_labels, all_preds, target_names=['real', 'fake']))"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ia-0vHPD1Mh","executionInfo":{"status":"ok","timestamp":1747834690724,"user_tz":-540,"elapsed":99239,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"4af46ae2-902d-46f1-8e63-2b9894b82cc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","100%|██████████| 120/120 [01:37<00:00,  1.23it/s]"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.525\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","        real       0.52      0.82      0.63        60\n","        fake       0.56      0.23      0.33        60\n","\n","    accuracy                           0.53       120\n","   macro avg       0.54      0.53      0.48       120\n","weighted avg       0.54      0.53      0.48       120\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models, transforms\n","import os\n","import numpy as np\n","import cv2\n","import glob\n","import random\n","\n","# Step 1: 디바이스 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"✅ Using device: {device}\")\n","\n","# Step 2: 모델 정의\n","class Model(nn.Module):\n","    def __init__(self, num_classes=2, latent_dim=1280, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n","        super(Model, self).__init__()\n","        base = models.efficientnet_b0(pretrained=True)\n","        self.features = base.features\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n","        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)\n","        self.dropout = nn.Dropout(0.4)\n","\n","    def forward(self, x):\n","        B, T, C, H, W = x.shape\n","        x = x.view(B * T, C, H, W)\n","        x = self.features(x)\n","        x = self.avgpool(x).view(B, T, -1)\n","        x, _ = self.lstm(x)\n","        out = self.fc(self.dropout(x.mean(dim=1)))\n","        return out\n","\n","# Step 3: 모델 및 옵티마이저 초기화\n","model = Model().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","# Step 4: 체크포인트 로드\n","checkpoint_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/checkpoints/best_checkpoint_FF.pth'  # ✅ 여기를 실제 경로로 수정하세요\n","checkpoint = torch.load(f'{checkpoint_path}', map_location=device)\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","model.eval()\n","print(\"✅ Model loaded and set to eval mode.\")\n","\n","# Step 5: 전처리 정의\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((112, 112)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])\n","\n","# Step 6: 테스트 비디오 로드\n","test_input_file_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/FF/validation/fake'    # ✅ 수정 필요\n","test_output_file_path = '/content/drive/MyDrive/class/capstone_com/datasetbackup/FF/validation/real'  # ✅ 수정 필요\n","frames = 32  # 프레임 수 임계값 (예시)\n","\n","new_video_files = glob.glob(f'{test_input_file_path}/*.mp4') + glob.glob(f'{test_output_file_path}/*.mp4')\n","random.shuffle(new_video_files)\n","\n","frame_count = []\n","short_frame = []\n","\n","for video_file in reversed(new_video_files):  # 뒤에서부터 삭제하기 위해 reversed\n","    cap = cv2.VideoCapture(video_file)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if total_frames < frames:\n","        new_video_files.remove(video_file)\n","        short_frame.append(video_file)\n","        continue\n","    frame_count.append(total_frames)\n","    cap.release()\n","\n","print(\"🎞️ Frame counts:\", frame_count)\n","print(\"📁 Total valid videos:\", len(frame_count))\n","print(\"📊 Average frames per video:\", np.mean(frame_count))\n","print(\"⛔ Short videos removed:\", len(short_frame))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pDzbMJ9GuTa","executionInfo":{"status":"ok","timestamp":1747835814200,"user_tz":-540,"elapsed":1572,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"9a51d808-8c79-478c-d7b9-d53c4a3ecc66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["✅ Model loaded and set to eval mode.\n","🎞️ Frame counts: [148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148, 148]\n","📁 Total valid videos: 120\n","📊 Average frames per video: 148.0\n","⛔ Short videos removed: 0\n"]}]},{"cell_type":"code","source":["# === 수정된 Dataset ===\n","class ValidationVideoDataset(Dataset):\n","    def __init__(self, root_dir, transform=None, frames_required=100):\n","        self.samples = []\n","        self.transform = transform\n","        self.frames_required = frames_required\n","\n","        for label_name in ['real', 'fake']:\n","            class_dir = os.path.join(root_dir, label_name)\n","            if not os.path.isdir(class_dir):\n","                print(f\"[Warning] Directory not found: {class_dir}\")\n","                continue\n","            for filename in os.listdir(class_dir):\n","                if filename.endswith('.mp4'):\n","                    self.samples.append((os.path.join(class_dir, filename), label_map[label_name]))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        video_path, label = self.samples[idx]\n","        frames = extract_frames_from_video(video_path, self.frames_required)\n","\n","        if not frames:\n","            raise RuntimeError(f\"No frames extracted from video: {video_path}\")\n","\n","        transformed_frames = [self.transform(frame) for frame in frames]\n","        frames_tensor = torch.stack(transformed_frames)  # Shape: [T, C, H, W]\n","        return frames_tensor, label, video_path\n","\n","# === 평가 루프 수정 ===\n","all_preds = []\n","all_labels = []\n","all_scores = []\n","video_paths = []\n","\n","with torch.no_grad():\n","    for frames, label, path in tqdm(val_loader):\n","        # frames shape: [1, T, C, H, W] → [B, T, C, H, W]\n","        frames = frames.to(device)\n","        label = torch.tensor([label], dtype=torch.long).to(device)  # label -> [1]\n","\n","        output = model(frames)  # output shape: [1, 2]\n","        pred = torch.argmax(output, dim=1).item()\n","        prob_fake = torch.softmax(output, dim=1)[0, 1].item()\n","\n","        all_preds.append(pred)\n","        all_labels.append(label.item())\n","        all_scores.append(prob_fake)\n","        video_paths.append(path[0])  # path is a tuple of 1 item (from batch_size=1)\n","\n","# === 결과 출력 및 저장 ===\n","from sklearn.metrics import classification_report, accuracy_score\n","import pandas as pd\n","\n","print(\"✅ Accuracy:\", accuracy_score(all_labels, all_preds))\n","print(\"\\n📊 Classification Report:\")\n","print(classification_report(all_labels, all_preds, target_names=['real', 'fake']))\n","\n","# 결과 저장\n","results_dir = '/content/drive/MyDrive/class/capstone_com/results'\n","os.makedirs(results_dir, exist_ok=True)\n","\n","results_df = pd.DataFrame({\n","    'video_path': video_paths,\n","    'true_label': all_labels,\n","    'predicted_label': all_preds,\n","    'fake_probability': all_scores\n","})\n","results_df.to_csv(os.path.join(results_dir, 'test_predictions.csv'), index=False)\n","\n","# 오분류 분석\n","misclassified = results_df[results_df['true_label'] != results_df['predicted_label']]\n","print(f\"\\n❌ Number of misclassified videos: {len(misclassified)} out of {len(results_df)}\")\n","\n","if len(misclassified) > 0:\n","    print(\"\\n🔍 Sample of misclassified videos:\")\n","    for i, (_, row) in enumerate(misclassified.head(5).iterrows()):\n","        print(f\"📼 Video: {row['video_path']}\")\n","        print(f\"   ✅ True label: {'Fake' if row['true_label'] == 1 else 'Real'}\")\n","        print(f\"   ❌ Predicted: {'Fake' if row['predicted_label'] == 1 else 'Real'}\")\n","        print(f\"   🔢 Fake probability: {row['fake_probability']:.4f}\")\n","        print(\"---\")\n","\n","print(f\"\\n✅ Evaluation complete. Results saved to: {results_dir}\")\n","print(\"\\n🏁 Deepfake detection model training and evaluation completed successfully!\")\n"],"metadata":{"id":"6OKdBBu3LCbv","executionInfo":{"status":"error","timestamp":1747836482455,"user_tz":-540,"elapsed":908,"user":{"displayName":"이은서","userId":"09202701988006085748"}},"outputId":"377d97e0-b218-48ef-8858-c9342a9950a8","colab":{"base_uri":"https://localhost:8080/","height":233}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/120 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"not enough values to unpack (expected 3, got 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-92f46ca874fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# frames shape: [1, T, C, H, W] → [B, T, C, H, W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"]}]}]}