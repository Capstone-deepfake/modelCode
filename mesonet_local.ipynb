{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Complete Test Script for MesoNet+LSTM Deepfake Detection\n",
        "Test your trained model on a single example from your dataset\n",
        "Just run this entire script in Google Colab!\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "##############################################\n",
        "# MODEL ARCHITECTURE (Exact copy from training)\n",
        "##############################################\n",
        "\n",
        "class EnhancedMesoNet(nn.Module):\n",
        "    \"\"\"Enhanced MesoNet with adaptive architecture\"\"\"\n",
        "    def __init__(self, image_size=128):\n",
        "        super(EnhancedMesoNet, self).__init__()\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Third conv block\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fourth conv block\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fifth conv block\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1, bias=False)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.pool5 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Calculate feature dimension\n",
        "        feature_dim = 256 * (image_size // 32) * (image_size // 32)\n",
        "        self.feature_size = feature_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.pool5(F.relu(self.bn5(self.conv5(x))))\n",
        "        return x\n",
        "\n",
        "\n",
        "class EnhancedMesoNetLSTM(nn.Module):\n",
        "    \"\"\"Enhanced MesoNet + LSTM with adaptive parameters\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(EnhancedMesoNetLSTM, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        # Enhanced MesoNet base model\n",
        "        self.mesonet = EnhancedMesoNet(config['image_size'])\n",
        "        feature_dim = self.mesonet.feature_size\n",
        "\n",
        "        # Flatten features\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Feature reduction layer\n",
        "        self.feature_reducer = nn.Sequential(\n",
        "            nn.Linear(feature_dim, config['lstm_hidden_size']),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(config['lstm_hidden_size']),\n",
        "            nn.Dropout(config['dropout_rate'] * 0.5)\n",
        "        )\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=config['lstm_hidden_size'],\n",
        "            hidden_size=config['lstm_hidden_size'],\n",
        "            num_layers=config['lstm_layers'],\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=config['dropout_rate'] if config['lstm_layers'] > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=config['lstm_hidden_size'] * 2,\n",
        "            num_heads=8,\n",
        "            dropout=config['dropout_rate'],\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Final classification layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(config['dropout_rate']),\n",
        "            nn.Linear(config['lstm_hidden_size'] * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(config['dropout_rate'] * 0.5),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config['dropout_rate'] * 0.3),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch, frames, channels, height, width]\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        # Process each frame through MesoNet\n",
        "        frame_features = []\n",
        "        for t in range(seq_len):\n",
        "            frame = x[:, t, :, :, :]\n",
        "            features = self.mesonet(frame)\n",
        "            features = self.flatten(features)\n",
        "            features = self.feature_reducer(features)\n",
        "            frame_features.append(features)\n",
        "\n",
        "        # Stack features for LSTM\n",
        "        lstm_input = torch.stack(frame_features, dim=1)\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(lstm_input)\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attended_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "\n",
        "        # Global average pooling over sequence dimension\n",
        "        pooled_out = torch.mean(attended_out, dim=1)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.classifier(pooled_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "##############################################\n",
        "# DEEPFAKE DETECTOR CLASS\n",
        "##############################################\n",
        "\n",
        "class DeepfakeDetector:\n",
        "    \"\"\"Professional deepfake detection system\"\"\"\n",
        "\n",
        "    def __init__(self, model_path, device=None):\n",
        "        \"\"\"\n",
        "        Initialize the deepfake detector\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to model_for_local_inference.pth\n",
        "            device (str): Device to use ('cuda', 'cpu', or None for auto)\n",
        "        \"\"\"\n",
        "        if device is None:\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        print(f\"ğŸš€ Initializing MesoNet+LSTM Deepfake Detector\")\n",
        "        print(f\"   Device: {self.device}\")\n",
        "\n",
        "        self.model = None\n",
        "        self.config = None\n",
        "        self.load_model(model_path)\n",
        "        self.setup_transforms()\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load model from state dict file\"\"\"\n",
        "\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "        print(f\"ğŸ“ Loading model from: {os.path.basename(model_path)}\")\n",
        "\n",
        "        try:\n",
        "            # Load model data with weights_only=False for compatibility\n",
        "            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)\n",
        "\n",
        "            # Extract configuration and create model\n",
        "            self.config = checkpoint['config']\n",
        "            self.model = EnhancedMesoNetLSTM(self.config)\n",
        "\n",
        "            # Load trained weights\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "            # Move to device and set evaluation mode\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            accuracy = checkpoint.get('accuracy', 'Unknown')\n",
        "\n",
        "            print(f\"âœ… Model loaded successfully!\")\n",
        "            print(f\"   Configuration: {self.config['name']}\")\n",
        "            print(f\"   Accuracy: {accuracy:.4f} (91.06% on DFDC dataset)\")\n",
        "            print(f\"   Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "            print(f\"   Frame count: {self.config['frame_count']}\")\n",
        "            print(f\"   Image size: {self.config['image_size']}x{self.config['image_size']}\")\n",
        "            print(f\"   LSTM hidden size: {self.config['lstm_hidden_size']}\")\n",
        "            print(f\"   LSTM layers: {self.config['lstm_layers']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model: {str(e)}\")\n",
        "\n",
        "    def setup_transforms(self):\n",
        "        \"\"\"Setup image preprocessing transforms\"\"\"\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def extract_frames(self, video_path):\n",
        "        \"\"\"Extract frames from video file\"\"\"\n",
        "\n",
        "        print(f\"ğŸ“¹ Opening video: {os.path.basename(video_path)}\")\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"Could not open video: {video_path}\")\n",
        "\n",
        "        frames = []\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        duration = frame_count / fps if fps > 0 else 0\n",
        "\n",
        "        print(f\"   ğŸ“Š Video info: {frame_count} frames, {fps:.1f} FPS, {duration:.1f}s\")\n",
        "\n",
        "        if frame_count <= 0:\n",
        "            cap.release()\n",
        "            raise ValueError(f\"No frames found in video\")\n",
        "\n",
        "        # Calculate frame indices to extract (evenly distributed)\n",
        "        indices = np.linspace(0, frame_count - 1, self.config['frame_count'], dtype=int)\n",
        "\n",
        "        print(f\"   ğŸ¬ Extracting {self.config['frame_count']} frames...\")\n",
        "\n",
        "        for idx in indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if ret:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = cv2.resize(frame, (self.config['image_size'], self.config['image_size']))\n",
        "                frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Pad with last frame if needed\n",
        "        while len(frames) < self.config['frame_count']:\n",
        "            if len(frames) > 0:\n",
        "                frames.append(frames[-1])\n",
        "            else:\n",
        "                frames.append(np.zeros((self.config['image_size'], self.config['image_size'], 3), dtype=np.uint8))\n",
        "\n",
        "        print(f\"   âœ… Extracted {len(frames)} frames\")\n",
        "        return np.array(frames)\n",
        "\n",
        "    def predict(self, video_path):\n",
        "        \"\"\"\n",
        "        Predict if a video is fake or real\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to the video file\n",
        "\n",
        "        Returns:\n",
        "            dict: Prediction results\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"\\nğŸ” Analyzing: {os.path.basename(video_path)}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Extract and preprocess frames\n",
        "        frames = self.extract_frames(video_path)\n",
        "\n",
        "        # Transform frames\n",
        "        print(f\"   ğŸ”„ Preprocessing frames...\")\n",
        "        transformed_frames = []\n",
        "        for frame in frames:\n",
        "            frame_tensor = self.transform(frame)\n",
        "            transformed_frames.append(frame_tensor)\n",
        "\n",
        "        # Create batch tensor\n",
        "        video_tensor = torch.stack(transformed_frames).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # Run inference\n",
        "        print(f\"   ğŸ§  Running inference on {self.device}...\")\n",
        "        with torch.no_grad():\n",
        "            output = self.model(video_tensor)\n",
        "            probability = torch.sigmoid(output).item()\n",
        "\n",
        "        # Calculate results\n",
        "        prediction = \"FAKE\" if probability > 0.5 else \"REAL\"\n",
        "        confidence = max(probability, 1 - probability)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        result = {\n",
        "            'prediction': prediction,\n",
        "            'fake_probability': probability,\n",
        "            'real_probability': 1 - probability,\n",
        "            'confidence': confidence,\n",
        "            'processing_time': processing_time\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "##############################################\n",
        "# DATASET INTEGRATION FUNCTIONS\n",
        "##############################################\n",
        "\n",
        "def load_dataset_metadata(base_path=\"/content/drive/MyDrive/Dataset-3\"):\n",
        "    \"\"\"Load the dataset metadata for random video selection\"\"\"\n",
        "\n",
        "    csv_path = os.path.join(base_path, \"global_metadata_cleaned.csv\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"Dataset metadata not found: {csv_path}\")\n",
        "\n",
        "    print(f\"ğŸ“Š Loading dataset metadata from: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    print(f\"âœ… Loaded {len(df)} total samples\")\n",
        "    print(f\"ğŸ“ˆ Available datasets: {list(df['dataset'].unique())}\")\n",
        "    print(f\"ğŸ·ï¸  Label distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    print(f\"ğŸ“‚ Subset distribution:\")\n",
        "    print(df['subset'].value_counts())\n",
        "\n",
        "    return df, base_path\n",
        "\n",
        "def get_random_video(df, base_path, dataset='DFDC', subset='test', label=None):\n",
        "    \"\"\"\n",
        "    Get one random video from the dataset\n",
        "\n",
        "    Args:\n",
        "        df: Dataset metadata DataFrame\n",
        "        base_path: Base path to dataset\n",
        "        dataset: Dataset name ('DFDC', 'FF', etc.)\n",
        "        subset: 'train' or 'test'\n",
        "        label: 'REAL', 'FAKE', or None for both\n",
        "\n",
        "    Returns:\n",
        "        tuple: (video_path, video_info)\n",
        "    \"\"\"\n",
        "\n",
        "    # Filter dataset\n",
        "    filtered_df = df[df['dataset'] == dataset]\n",
        "    if subset:\n",
        "        filtered_df = filtered_df[filtered_df['subset'] == subset]\n",
        "    if label:\n",
        "        filtered_df = filtered_df[filtered_df['label'] == label]\n",
        "\n",
        "    if len(filtered_df) == 0:\n",
        "        raise ValueError(f\"No videos found with criteria: dataset={dataset}, subset={subset}, label={label}\")\n",
        "\n",
        "    # Sample one random video\n",
        "    sampled_video = filtered_df.sample(n=1, random_state=None).iloc[0]\n",
        "\n",
        "    # Get full path and info\n",
        "    video_path = os.path.join(base_path, sampled_video['file_path'])\n",
        "\n",
        "    video_info = {\n",
        "        'path': video_path,\n",
        "        'filename': os.path.basename(video_path),\n",
        "        'label': sampled_video['label'],\n",
        "        'dataset': sampled_video['dataset'],\n",
        "        'subset': sampled_video['subset']\n",
        "    }\n",
        "\n",
        "    print(f\"ğŸ² Selected random video from {dataset} {subset} set:\")\n",
        "    print(f\"   ğŸ“¹ {video_info['filename']} - {video_info['label']}\")\n",
        "\n",
        "    return video_path, video_info\n",
        "\n",
        "def print_result_with_ground_truth(video_info, result):\n",
        "    \"\"\"Print results with ground truth comparison\"\"\"\n",
        "\n",
        "    print(f\"\\nğŸ¯ RESULTS for {video_info['filename']}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Ground truth vs prediction\n",
        "    gt_emoji = \"âœ…\" if video_info['label'] == \"REAL\" else \"ğŸš¨\"\n",
        "    pred_emoji = \"âœ…\" if result['prediction'] == \"REAL\" else \"ğŸš¨\"\n",
        "    correct = result['prediction'] == video_info['label']\n",
        "    correct_emoji = \"âœ…\" if correct else \"âŒ\"\n",
        "\n",
        "    print(f\"ğŸ·ï¸  Ground Truth: {gt_emoji} {video_info['label']}\")\n",
        "    print(f\"ğŸ¤– Prediction: {pred_emoji} {result['prediction']}\")\n",
        "    print(f\"ğŸ¯ Correct: {correct_emoji} {correct}\")\n",
        "    print(f\"ğŸ² Confidence: {result['confidence']:.4f} ({result['confidence']*100:.1f}%)\")\n",
        "\n",
        "    # Detailed probabilities\n",
        "    print(f\"\\nğŸ“Š Detailed Analysis:\")\n",
        "    print(f\"   Fake probability: {result['fake_probability']:.4f} ({result['fake_probability']*100:.1f}%)\")\n",
        "    print(f\"   Real probability: {result['real_probability']:.4f} ({result['real_probability']*100:.1f}%)\")\n",
        "    print(f\"   Processing time: {result['processing_time']:.2f}s\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(f\"\\nğŸ’¡ Interpretation:\")\n",
        "    if result['confidence'] > 0.9:\n",
        "        confidence_level = \"Very High\"\n",
        "    elif result['confidence'] > 0.8:\n",
        "        confidence_level = \"High\"\n",
        "    elif result['confidence'] > 0.7:\n",
        "        confidence_level = \"Medium\"\n",
        "    else:\n",
        "        confidence_level = \"Low\"\n",
        "\n",
        "    print(f\"   Confidence Level: {confidence_level}\")\n",
        "\n",
        "    if result['prediction'] == \"FAKE\":\n",
        "        print(f\"   âš ï¸  This video appears to be artificially generated or manipulated\")\n",
        "    else:\n",
        "        print(f\"   âœ… This video appears to be authentic\")\n",
        "\n",
        "    # Performance assessment\n",
        "    if correct:\n",
        "        print(f\"   ğŸ‰ Model prediction is CORRECT!\")\n",
        "    else:\n",
        "        print(f\"   ğŸ˜ Model prediction is INCORRECT\")\n",
        "\n",
        "    return correct\n",
        "\n",
        "##############################################\n",
        "# MAIN TESTING FUNCTION\n",
        "##############################################\n",
        "\n",
        "def test_single_video_from_dataset():\n",
        "    \"\"\"Test the model on a single video from the dataset\"\"\"\n",
        "\n",
        "    print(\"ğŸ¬ MesoNet+LSTM Single Video Test\")\n",
        "    print(\"ğŸ“ˆ Trained accuracy: 91.06% on DFDC dataset\")\n",
        "    print(\"ğŸ² Testing on one random video from your training dataset\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Set paths\n",
        "    base_path = \"/content/drive/MyDrive/Dataset-3\"\n",
        "    model_path = \"/content/drive/MyDrive/Dataset-3/dfdc_training_run_20250527_143110/model_for_local_inference.pth\"\n",
        "\n",
        "    try:\n",
        "        # Load dataset metadata\n",
        "        df, base_path = load_dataset_metadata(base_path)\n",
        "\n",
        "        # Get one random video from test set\n",
        "        video_path, video_info = get_random_video(\n",
        "            df, base_path,\n",
        "            dataset='DFDC',\n",
        "            subset='test'  # Use test set for fair evaluation\n",
        "        )\n",
        "\n",
        "        # Check if video file exists\n",
        "        if not os.path.exists(video_path):\n",
        "            print(f\"âŒ Video file not found: {video_path}\")\n",
        "            return\n",
        "\n",
        "        # Initialize detector\n",
        "        detector = DeepfakeDetector(model_path)\n",
        "\n",
        "        # Run prediction\n",
        "        result = detector.predict(video_path)\n",
        "\n",
        "        # Print results with ground truth comparison\n",
        "        correct = print_result_with_ground_truth(video_info, result)\n",
        "\n",
        "        # Final summary\n",
        "        print(f\"\\nğŸ“‹ TEST SUMMARY:\")\n",
        "        print(\"=\" * 30)\n",
        "        print(f\"Video: {video_info['filename']}\")\n",
        "        print(f\"Ground Truth: {video_info['label']}\")\n",
        "        print(f\"Prediction: {result['prediction']}\")\n",
        "        print(f\"Correct: {'âœ… YES' if correct else 'âŒ NO'}\")\n",
        "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "        print(f\"Processing Time: {result['processing_time']:.2f}s\")\n",
        "\n",
        "        if correct:\n",
        "            print(f\"\\nğŸ‰ SUCCESS! Your model correctly identified this video!\")\n",
        "        else:\n",
        "            print(f\"\\nğŸ˜ Your model made an incorrect prediction on this video.\")\n",
        "            print(f\"ğŸ’¡ This is normal - even 91.06% accuracy means some errors occur.\")\n",
        "\n",
        "        return result, video_info, correct\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during testing: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, False\n",
        "\n",
        "##############################################\n",
        "# RUN THE TEST\n",
        "##############################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸš€ Starting single video test...\")\n",
        "    print(\"ğŸ”„ This will test your 91.06% accuracy model on one random video from your dataset\")\n",
        "    print()\n",
        "\n",
        "    # Run the test\n",
        "    result, video_info, correct = test_single_video_from_dataset()\n",
        "\n",
        "    if result is not None:\n",
        "        print(f\"\\nâœ… Test completed successfully!\")\n",
        "        print(f\"ğŸ¯ Your model's prediction was {'CORRECT' if correct else 'INCORRECT'}\")\n",
        "    else:\n",
        "        print(f\"\\nâŒ Test failed - check the error messages above\")\n",
        "\n",
        "    print(f\"\\nğŸ Test finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VKA8RGHdSW0",
        "outputId": "371f2610-77e0-485a-d41a-4db2432b56d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ğŸš€ Starting single video test...\n",
            "ğŸ”„ This will test your 91.06% accuracy model on one random video from your dataset\n",
            "\n",
            "ğŸ¬ MesoNet+LSTM Single Video Test\n",
            "ğŸ“ˆ Trained accuracy: 91.06% on DFDC dataset\n",
            "ğŸ² Testing on one random video from your training dataset\n",
            "======================================================================\n",
            "ğŸ“Š Loading dataset metadata from: /content/drive/MyDrive/Dataset-3/global_metadata_cleaned.csv\n",
            "âœ… Loaded 5281 total samples\n",
            "ğŸ“ˆ Available datasets: ['DFDC', 'FF']\n",
            "ğŸ·ï¸  Label distribution:\n",
            "label\n",
            "REAL    2719\n",
            "FAKE    2562\n",
            "Name: count, dtype: int64\n",
            "ğŸ“‚ Subset distribution:\n",
            "subset\n",
            "train    4224\n",
            "test     1057\n",
            "Name: count, dtype: int64\n",
            "ğŸ² Selected random video from DFDC test set:\n",
            "   ğŸ“¹ bztdemptfg.mp4 - FAKE\n",
            "ğŸš€ Initializing MesoNet+LSTM Deepfake Detector\n",
            "   Device: cpu\n",
            "ğŸ“ Loading model from: model_for_local_inference.pth\n",
            "âœ… Model loaded successfully!\n",
            "   Configuration: base_config\n",
            "   Accuracy: 0.9106 (91.06% on DFDC dataset)\n",
            "   Parameters: 5,271,057\n",
            "   Frame count: 32\n",
            "   Image size: 128x128\n",
            "   LSTM hidden size: 256\n",
            "   LSTM layers: 2\n",
            "\n",
            "ğŸ” Analyzing: bztdemptfg.mp4\n",
            "ğŸ“¹ Opening video: bztdemptfg.mp4\n",
            "   ğŸ“Š Video info: 148 frames, 30.0 FPS, 4.9s\n",
            "   ğŸ¬ Extracting 32 frames...\n",
            "   âœ… Extracted 32 frames\n",
            "   ğŸ”„ Preprocessing frames...\n",
            "   ğŸ§  Running inference on cpu...\n",
            "\n",
            "ğŸ¯ RESULTS for bztdemptfg.mp4\n",
            "============================================================\n",
            "ğŸ·ï¸  Ground Truth: ğŸš¨ FAKE\n",
            "ğŸ¤– Prediction: ğŸš¨ FAKE\n",
            "ğŸ¯ Correct: âœ… True\n",
            "ğŸ² Confidence: 0.9996 (100.0%)\n",
            "\n",
            "ğŸ“Š Detailed Analysis:\n",
            "   Fake probability: 0.9996 (100.0%)\n",
            "   Real probability: 0.0004 (0.0%)\n",
            "   Processing time: 0.66s\n",
            "\n",
            "ğŸ’¡ Interpretation:\n",
            "   Confidence Level: Very High\n",
            "   âš ï¸  This video appears to be artificially generated or manipulated\n",
            "   ğŸ‰ Model prediction is CORRECT!\n",
            "\n",
            "ğŸ“‹ TEST SUMMARY:\n",
            "==============================\n",
            "Video: bztdemptfg.mp4\n",
            "Ground Truth: FAKE\n",
            "Prediction: FAKE\n",
            "Correct: âœ… YES\n",
            "Confidence: 0.9996\n",
            "Processing Time: 0.66s\n",
            "\n",
            "ğŸ‰ SUCCESS! Your model correctly identified this video!\n",
            "\n",
            "âœ… Test completed successfully!\n",
            "ğŸ¯ Your model's prediction was CORRECT\n",
            "\n",
            "ğŸ Test finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_scYM0iRd7vI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}