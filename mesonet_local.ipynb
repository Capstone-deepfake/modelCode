{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Complete Test Script for MesoNet+LSTM Deepfake Detection\n",
        "Test your trained model on a single example from your dataset\n",
        "Just run this entire script in Google Colab!\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "##############################################\n",
        "# MODEL ARCHITECTURE (Exact copy from training)\n",
        "##############################################\n",
        "\n",
        "class EnhancedMesoNet(nn.Module):\n",
        "    \"\"\"Enhanced MesoNet with adaptive architecture\"\"\"\n",
        "    def __init__(self, image_size=128):\n",
        "        super(EnhancedMesoNet, self).__init__()\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # First conv block\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Second conv block\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Third conv block\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fourth conv block\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fifth conv block\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1, bias=False)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.pool5 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Calculate feature dimension\n",
        "        feature_dim = 256 * (image_size // 32) * (image_size // 32)\n",
        "        self.feature_size = feature_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.pool5(F.relu(self.bn5(self.conv5(x))))\n",
        "        return x\n",
        "\n",
        "\n",
        "class EnhancedMesoNetLSTM(nn.Module):\n",
        "    \"\"\"Enhanced MesoNet + LSTM with adaptive parameters\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(EnhancedMesoNetLSTM, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        # Enhanced MesoNet base model\n",
        "        self.mesonet = EnhancedMesoNet(config['image_size'])\n",
        "        feature_dim = self.mesonet.feature_size\n",
        "\n",
        "        # Flatten features\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Feature reduction layer\n",
        "        self.feature_reducer = nn.Sequential(\n",
        "            nn.Linear(feature_dim, config['lstm_hidden_size']),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(config['lstm_hidden_size']),\n",
        "            nn.Dropout(config['dropout_rate'] * 0.5)\n",
        "        )\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=config['lstm_hidden_size'],\n",
        "            hidden_size=config['lstm_hidden_size'],\n",
        "            num_layers=config['lstm_layers'],\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=config['dropout_rate'] if config['lstm_layers'] > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=config['lstm_hidden_size'] * 2,\n",
        "            num_heads=8,\n",
        "            dropout=config['dropout_rate'],\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Final classification layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(config['dropout_rate']),\n",
        "            nn.Linear(config['lstm_hidden_size'] * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(config['dropout_rate'] * 0.5),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config['dropout_rate'] * 0.3),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch, frames, channels, height, width]\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        # Process each frame through MesoNet\n",
        "        frame_features = []\n",
        "        for t in range(seq_len):\n",
        "            frame = x[:, t, :, :, :]\n",
        "            features = self.mesonet(frame)\n",
        "            features = self.flatten(features)\n",
        "            features = self.feature_reducer(features)\n",
        "            frame_features.append(features)\n",
        "\n",
        "        # Stack features for LSTM\n",
        "        lstm_input = torch.stack(frame_features, dim=1)\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(lstm_input)\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attended_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "\n",
        "        # Global average pooling over sequence dimension\n",
        "        pooled_out = torch.mean(attended_out, dim=1)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.classifier(pooled_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "##############################################\n",
        "# DEEPFAKE DETECTOR CLASS\n",
        "##############################################\n",
        "\n",
        "class DeepfakeDetector:\n",
        "    \"\"\"Professional deepfake detection system\"\"\"\n",
        "\n",
        "    def __init__(self, model_path, device=None):\n",
        "        \"\"\"\n",
        "        Initialize the deepfake detector\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to model_for_local_inference.pth\n",
        "            device (str): Device to use ('cuda', 'cpu', or None for auto)\n",
        "        \"\"\"\n",
        "        if device is None:\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        print(f\"üöÄ Initializing MesoNet+LSTM Deepfake Detector\")\n",
        "        print(f\"   Device: {self.device}\")\n",
        "\n",
        "        self.model = None\n",
        "        self.config = None\n",
        "        self.load_model(model_path)\n",
        "        self.setup_transforms()\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load model from state dict file\"\"\"\n",
        "\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "        print(f\"üìÅ Loading model from: {os.path.basename(model_path)}\")\n",
        "\n",
        "        try:\n",
        "            # Load model data with weights_only=False for compatibility\n",
        "            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)\n",
        "\n",
        "            # Extract configuration and create model\n",
        "            self.config = checkpoint['config']\n",
        "            self.model = EnhancedMesoNetLSTM(self.config)\n",
        "\n",
        "            # Load trained weights\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "            # Move to device and set evaluation mode\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            accuracy = checkpoint.get('accuracy', 'Unknown')\n",
        "\n",
        "            print(f\"‚úÖ Model loaded successfully!\")\n",
        "            print(f\"   Configuration: {self.config['name']}\")\n",
        "            print(f\"   Accuracy: {accuracy:.4f} (91.06% on DFDC dataset)\")\n",
        "            print(f\"   Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "            print(f\"   Frame count: {self.config['frame_count']}\")\n",
        "            print(f\"   Image size: {self.config['image_size']}x{self.config['image_size']}\")\n",
        "            print(f\"   LSTM hidden size: {self.config['lstm_hidden_size']}\")\n",
        "            print(f\"   LSTM layers: {self.config['lstm_layers']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model: {str(e)}\")\n",
        "\n",
        "    def setup_transforms(self):\n",
        "        \"\"\"Setup image preprocessing transforms\"\"\"\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def extract_frames(self, video_path):\n",
        "        \"\"\"Extract frames from video file\"\"\"\n",
        "\n",
        "        print(f\"üìπ Opening video: {os.path.basename(video_path)}\")\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"Could not open video: {video_path}\")\n",
        "\n",
        "        frames = []\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        duration = frame_count / fps if fps > 0 else 0\n",
        "\n",
        "        print(f\"   üìä Video info: {frame_count} frames, {fps:.1f} FPS, {duration:.1f}s\")\n",
        "\n",
        "        if frame_count <= 0:\n",
        "            cap.release()\n",
        "            raise ValueError(f\"No frames found in video\")\n",
        "\n",
        "        # Calculate frame indices to extract (evenly distributed)\n",
        "        indices = np.linspace(0, frame_count - 1, self.config['frame_count'], dtype=int)\n",
        "\n",
        "        print(f\"   üé¨ Extracting {self.config['frame_count']} frames...\")\n",
        "\n",
        "        for idx in indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if ret:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = cv2.resize(frame, (self.config['image_size'], self.config['image_size']))\n",
        "                frames.append(frame)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Pad with last frame if needed\n",
        "        while len(frames) < self.config['frame_count']:\n",
        "            if len(frames) > 0:\n",
        "                frames.append(frames[-1])\n",
        "            else:\n",
        "                frames.append(np.zeros((self.config['image_size'], self.config['image_size'], 3), dtype=np.uint8))\n",
        "\n",
        "        print(f\"   ‚úÖ Extracted {len(frames)} frames\")\n",
        "        return np.array(frames)\n",
        "\n",
        "    def predict(self, video_path):\n",
        "        \"\"\"\n",
        "        Predict if a video is fake or real\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to the video file\n",
        "\n",
        "        Returns:\n",
        "            dict: Prediction results\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"\\nüîç Analyzing: {os.path.basename(video_path)}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Extract and preprocess frames\n",
        "        frames = self.extract_frames(video_path)\n",
        "\n",
        "        # Transform frames\n",
        "        print(f\"   üîÑ Preprocessing frames...\")\n",
        "        transformed_frames = []\n",
        "        for frame in frames:\n",
        "            frame_tensor = self.transform(frame)\n",
        "            transformed_frames.append(frame_tensor)\n",
        "\n",
        "        # Create batch tensor\n",
        "        video_tensor = torch.stack(transformed_frames).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # Run inference\n",
        "        print(f\"   üß† Running inference on {self.device}...\")\n",
        "        with torch.no_grad():\n",
        "            output = self.model(video_tensor)\n",
        "            probability = torch.sigmoid(output).item()\n",
        "\n",
        "        # Calculate results\n",
        "        prediction = \"FAKE\" if probability > 0.5 else \"REAL\"\n",
        "        confidence = max(probability, 1 - probability)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        result = {\n",
        "            'prediction': prediction,\n",
        "            'fake_probability': probability,\n",
        "            'real_probability': 1 - probability,\n",
        "            'confidence': confidence,\n",
        "            'processing_time': processing_time\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "##############################################\n",
        "# DATASET INTEGRATION FUNCTIONS\n",
        "##############################################\n",
        "\n",
        "def load_dataset_metadata(base_path=\"/content/drive/MyDrive/Dataset-3\"):\n",
        "    \"\"\"Load the dataset metadata for random video selection\"\"\"\n",
        "\n",
        "    csv_path = os.path.join(base_path, \"global_metadata_cleaned.csv\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"Dataset metadata not found: {csv_path}\")\n",
        "\n",
        "    print(f\"üìä Loading dataset metadata from: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(df)} total samples\")\n",
        "    print(f\"üìà Available datasets: {list(df['dataset'].unique())}\")\n",
        "    print(f\"üè∑Ô∏è  Label distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    print(f\"üìÇ Subset distribution:\")\n",
        "    print(df['subset'].value_counts())\n",
        "\n",
        "    return df, base_path\n",
        "\n",
        "def get_random_video(df, base_path, dataset='DFDC', subset='test', label=None):\n",
        "    \"\"\"\n",
        "    Get one random video from the dataset\n",
        "\n",
        "    Args:\n",
        "        df: Dataset metadata DataFrame\n",
        "        base_path: Base path to dataset\n",
        "        dataset: Dataset name ('DFDC', 'FF', etc.)\n",
        "        subset: 'train' or 'test'\n",
        "        label: 'REAL', 'FAKE', or None for both\n",
        "\n",
        "    Returns:\n",
        "        tuple: (video_path, video_info)\n",
        "    \"\"\"\n",
        "\n",
        "    # Filter dataset\n",
        "    filtered_df = df[df['dataset'] == dataset]\n",
        "    if subset:\n",
        "        filtered_df = filtered_df[filtered_df['subset'] == subset]\n",
        "    if label:\n",
        "        filtered_df = filtered_df[filtered_df['label'] == label]\n",
        "\n",
        "    if len(filtered_df) == 0:\n",
        "        raise ValueError(f\"No videos found with criteria: dataset={dataset}, subset={subset}, label={label}\")\n",
        "\n",
        "    # Sample one random video\n",
        "    sampled_video = filtered_df.sample(n=1, random_state=None).iloc[0]\n",
        "\n",
        "    # Get full path and info\n",
        "    video_path = os.path.join(base_path, sampled_video['file_path'])\n",
        "\n",
        "    video_info = {\n",
        "        'path': video_path,\n",
        "        'filename': os.path.basename(video_path),\n",
        "        'label': sampled_video['label'],\n",
        "        'dataset': sampled_video['dataset'],\n",
        "        'subset': sampled_video['subset']\n",
        "    }\n",
        "\n",
        "    print(f\"üé≤ Selected random video from {dataset} {subset} set:\")\n",
        "    print(f\"   üìπ {video_info['filename']} - {video_info['label']}\")\n",
        "\n",
        "    return video_path, video_info\n",
        "\n",
        "def print_result_with_ground_truth(video_info, result):\n",
        "    \"\"\"Print results with ground truth comparison\"\"\"\n",
        "\n",
        "    print(f\"\\nüéØ RESULTS for {video_info['filename']}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Ground truth vs prediction\n",
        "    gt_emoji = \"‚úÖ\" if video_info['label'] == \"REAL\" else \"üö®\"\n",
        "    pred_emoji = \"‚úÖ\" if result['prediction'] == \"REAL\" else \"üö®\"\n",
        "    correct = result['prediction'] == video_info['label']\n",
        "    correct_emoji = \"‚úÖ\" if correct else \"‚ùå\"\n",
        "\n",
        "    print(f\"üè∑Ô∏è  Ground Truth: {gt_emoji} {video_info['label']}\")\n",
        "    print(f\"ü§ñ Prediction: {pred_emoji} {result['prediction']}\")\n",
        "    print(f\"üéØ Correct: {correct_emoji} {correct}\")\n",
        "    print(f\"üé≤ Confidence: {result['confidence']:.4f} ({result['confidence']*100:.1f}%)\")\n",
        "\n",
        "    # Detailed probabilities\n",
        "    print(f\"\\nüìä Detailed Analysis:\")\n",
        "    print(f\"   Fake probability: {result['fake_probability']:.4f} ({result['fake_probability']*100:.1f}%)\")\n",
        "    print(f\"   Real probability: {result['real_probability']:.4f} ({result['real_probability']*100:.1f}%)\")\n",
        "    print(f\"   Processing time: {result['processing_time']:.2f}s\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(f\"\\nüí° Interpretation:\")\n",
        "    if result['confidence'] > 0.9:\n",
        "        confidence_level = \"Very High\"\n",
        "    elif result['confidence'] > 0.8:\n",
        "        confidence_level = \"High\"\n",
        "    elif result['confidence'] > 0.7:\n",
        "        confidence_level = \"Medium\"\n",
        "    else:\n",
        "        confidence_level = \"Low\"\n",
        "\n",
        "    print(f\"   Confidence Level: {confidence_level}\")\n",
        "\n",
        "    if result['prediction'] == \"FAKE\":\n",
        "        print(f\"   ‚ö†Ô∏è  This video appears to be artificially generated or manipulated\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ This video appears to be authentic\")\n",
        "\n",
        "    # Performance assessment\n",
        "    if correct:\n",
        "        print(f\"   üéâ Model prediction is CORRECT!\")\n",
        "    else:\n",
        "        print(f\"   üòû Model prediction is INCORRECT\")\n",
        "\n",
        "    return correct\n",
        "\n",
        "##############################################\n",
        "# MAIN TESTING FUNCTION\n",
        "##############################################\n",
        "\n",
        "def test_single_video_from_dataset():\n",
        "    \"\"\"Test the model on a single video from the dataset\"\"\"\n",
        "\n",
        "    print(\"üé¨ MesoNet+LSTM Single Video Test\")\n",
        "    print(\"üìà Trained accuracy: 91.06% on DFDC dataset\")\n",
        "    print(\"üé≤ Testing on one random video from your training dataset\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Set paths\n",
        "    base_path = \"/content/drive/MyDrive/Dataset-3\"\n",
        "    model_path = \"/content/drive/MyDrive/Dataset-3/dfdc_training_run_20250527_143110/model_for_local_inference.pth\"\n",
        "\n",
        "    try:\n",
        "        # Load dataset metadata\n",
        "        df, base_path = load_dataset_metadata(base_path)\n",
        "\n",
        "        # Get one random video from test set\n",
        "        video_path, video_info = get_random_video(\n",
        "            df, base_path,\n",
        "            dataset='DFDC',\n",
        "            subset='test'  # Use test set for fair evaluation\n",
        "        )\n",
        "\n",
        "        # Check if video file exists\n",
        "        if not os.path.exists(video_path):\n",
        "            print(f\"‚ùå Video file not found: {video_path}\")\n",
        "            return\n",
        "\n",
        "        # Initialize detector\n",
        "        detector = DeepfakeDetector(model_path)\n",
        "\n",
        "        # Run prediction\n",
        "        result = detector.predict(video_path)\n",
        "\n",
        "        # Print results with ground truth comparison\n",
        "        correct = print_result_with_ground_truth(video_info, result)\n",
        "\n",
        "        # Final summary\n",
        "        print(f\"\\nüìã TEST SUMMARY:\")\n",
        "        print(\"=\" * 30)\n",
        "        print(f\"Video: {video_info['filename']}\")\n",
        "        print(f\"Ground Truth: {video_info['label']}\")\n",
        "        print(f\"Prediction: {result['prediction']}\")\n",
        "        print(f\"Correct: {'‚úÖ YES' if correct else '‚ùå NO'}\")\n",
        "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "        print(f\"Processing Time: {result['processing_time']:.2f}s\")\n",
        "\n",
        "        if correct:\n",
        "            print(f\"\\nüéâ SUCCESS! Your model correctly identified this video!\")\n",
        "        else:\n",
        "            print(f\"\\nüòû Your model made an incorrect prediction on this video.\")\n",
        "            print(f\"üí° This is normal - even 91.06% accuracy means some errors occur.\")\n",
        "\n",
        "        return result, video_info, correct\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during testing: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, False\n",
        "\n",
        "##############################################\n",
        "# RUN THE TEST\n",
        "##############################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Starting single video test...\")\n",
        "    print(\"üîÑ This will test your 91.06% accuracy model on one random video from your dataset\")\n",
        "    print()\n",
        "\n",
        "    # Run the test\n",
        "    result, video_info, correct = test_single_video_from_dataset()\n",
        "\n",
        "    if result is not None:\n",
        "        print(f\"\\n‚úÖ Test completed successfully!\")\n",
        "        print(f\"üéØ Your model's prediction was {'CORRECT' if correct else 'INCORRECT'}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Test failed - check the error messages above\")\n",
        "\n",
        "    print(f\"\\nüèÅ Test finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VKA8RGHdSW0",
        "outputId": "371f2610-77e0-485a-d41a-4db2432b56d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üöÄ Starting single video test...\n",
            "üîÑ This will test your 91.06% accuracy model on one random video from your dataset\n",
            "\n",
            "üé¨ MesoNet+LSTM Single Video Test\n",
            "üìà Trained accuracy: 91.06% on DFDC dataset\n",
            "üé≤ Testing on one random video from your training dataset\n",
            "======================================================================\n",
            "üìä Loading dataset metadata from: /content/drive/MyDrive/Dataset-3/global_metadata_cleaned.csv\n",
            "‚úÖ Loaded 5281 total samples\n",
            "üìà Available datasets: ['DFDC', 'FF']\n",
            "üè∑Ô∏è  Label distribution:\n",
            "label\n",
            "REAL    2719\n",
            "FAKE    2562\n",
            "Name: count, dtype: int64\n",
            "üìÇ Subset distribution:\n",
            "subset\n",
            "train    4224\n",
            "test     1057\n",
            "Name: count, dtype: int64\n",
            "üé≤ Selected random video from DFDC test set:\n",
            "   üìπ bztdemptfg.mp4 - FAKE\n",
            "üöÄ Initializing MesoNet+LSTM Deepfake Detector\n",
            "   Device: cpu\n",
            "üìÅ Loading model from: model_for_local_inference.pth\n",
            "‚úÖ Model loaded successfully!\n",
            "   Configuration: base_config\n",
            "   Accuracy: 0.9106 (91.06% on DFDC dataset)\n",
            "   Parameters: 5,271,057\n",
            "   Frame count: 32\n",
            "   Image size: 128x128\n",
            "   LSTM hidden size: 256\n",
            "   LSTM layers: 2\n",
            "\n",
            "üîç Analyzing: bztdemptfg.mp4\n",
            "üìπ Opening video: bztdemptfg.mp4\n",
            "   üìä Video info: 148 frames, 30.0 FPS, 4.9s\n",
            "   üé¨ Extracting 32 frames...\n",
            "   ‚úÖ Extracted 32 frames\n",
            "   üîÑ Preprocessing frames...\n",
            "   üß† Running inference on cpu...\n",
            "\n",
            "üéØ RESULTS for bztdemptfg.mp4\n",
            "============================================================\n",
            "üè∑Ô∏è  Ground Truth: üö® FAKE\n",
            "ü§ñ Prediction: üö® FAKE\n",
            "üéØ Correct: ‚úÖ True\n",
            "üé≤ Confidence: 0.9996 (100.0%)\n",
            "\n",
            "üìä Detailed Analysis:\n",
            "   Fake probability: 0.9996 (100.0%)\n",
            "   Real probability: 0.0004 (0.0%)\n",
            "   Processing time: 0.66s\n",
            "\n",
            "üí° Interpretation:\n",
            "   Confidence Level: Very High\n",
            "   ‚ö†Ô∏è  This video appears to be artificially generated or manipulated\n",
            "   üéâ Model prediction is CORRECT!\n",
            "\n",
            "üìã TEST SUMMARY:\n",
            "==============================\n",
            "Video: bztdemptfg.mp4\n",
            "Ground Truth: FAKE\n",
            "Prediction: FAKE\n",
            "Correct: ‚úÖ YES\n",
            "Confidence: 0.9996\n",
            "Processing Time: 0.66s\n",
            "\n",
            "üéâ SUCCESS! Your model correctly identified this video!\n",
            "\n",
            "‚úÖ Test completed successfully!\n",
            "üéØ Your model's prediction was CORRECT\n",
            "\n",
            "üèÅ Test finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_scYM0iRd7vI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}